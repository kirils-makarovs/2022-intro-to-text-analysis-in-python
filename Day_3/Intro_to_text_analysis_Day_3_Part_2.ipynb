{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Intro_to_text_analysis_Day_3_Part_2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1J_VwH0BZl_NnrWBHnK-cScNcjzQWZXEG","authorship_tag":"ABX9TyOo4vu5zA5qyhjVETkaC6xx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EWB457PFT4J2"},"source":["# **Introduction to text analysis in Python. Day 3 Part 2**\n","\n","## *Dr Kirils Makarovs*\n","\n","## *k.makarovs@exeter.ac.uk*\n","\n","## *University of Exeter Q-Step Centre*\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"6b_C8WzNqV4z"},"source":["# **Welcome to Day 3 Part 2!**"]},{"cell_type":"markdown","metadata":{"id":"DmD95g7Na2wJ"},"source":["## **Today, we are going to look at:**\n","\n","+ Descriptive text analysis (continuation)\n","+ Text preprocessing\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ygnZsPQr3-mL"},"source":["# **Text preprocessing**\n","\n","**Text preprocessing** is all about preparing raw text entries for further modelling and analysis\n","\n","The better the text is preprocessed, the more accurate and reliable the results of the modelling are going to be!\n","\n","**Text preprocessing** might include different steps for different types of analysis, but here are some of the most common ones:\n","\n","<figure>\n","<left>\n","<img src=https://miro.medium.com/max/1024/1*pzjECYWP8WOWhwfCjebZVw.png  width=\"600\">\n","</figure>\n","\n","[Image source](https://medium.com/predict/how-does-nlp-pre-processing-actually-work-8d097c179af1)\n","\n","<figure>\n","<left>\n","<img src=https://iq.opengenus.org/content/images/2020/05/text_steps.png  width=\"600\">\n","</figure>\n","\n","[Image source](https://iq.opengenus.org/text-preprocessing-in-spacy/)"]},{"cell_type":"markdown","source":["## **Example: A single TED talk**"],"metadata":{"id":"iRMuXM_SrtyO"}},{"cell_type":"code","source":["# Importing some of the required libraries\n","\n","import pandas as pd\n","import numpy as np\n"],"metadata":{"id":"p5iZLg_Mske9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Uploading the dataset containing TED talks into the current Google Colab session\n","\n","from google.colab import files\n","\n","uploaded = files.upload()\n"],"metadata":{"id":"Dw8uB1aDsEYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting the dataset\n","\n","df = pd.read_csv('ted.csv')\n","\n","df\n"],"metadata":{"id":"3eu-p-UdsEYi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's take a single TED talk transcript and preprocess it!\n","\n","single_talk = df['transcript'][0]\n","\n","single_talk\n"],"metadata":{"id":"sSKy07Jgq5Tp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Step 1: Normalization**\n","\n","Note that this can mean much more than simply making all words lowercase (e.g. hashtag removal, HTML tag removal, etc.)\n","\n","However making everything lowercase is necessary so that such words as *Book* and *book* wouldn't be considered as separate entities (unless you really need it)"],"metadata":{"id":"Qs_V-JrztMXj"}},{"cell_type":"code","source":["single_talk_upd = single_talk.lower()\n","\n","single_talk_upd\n"],"metadata":{"id":"aNmhzUjRtR9Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Step 2: Tokenization**\n","\n","[Token](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html#:~:text=A%20token%20is%20an%20instance,containing%20the%20same%20character%20sequence.) - *is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing*\n","\n","Put it simply - before building a model, we need to chop down our document/documents into comparable pieces (you can think of them simply as words at this point). Punctuation is usually being removed at some point, unless it poses some specific meaning for the research\n","\n","Tokenization is a more subtle and precise way of splitting text into words like we previously did with `.split()` method\n","\n","<figure>\n","<left>\n","<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png  width=\"500\">\n","</figure>\n"],"metadata":{"id":"vbrwIoHruGPX"}},{"cell_type":"code","source":["# We will use spacy library for this and other tasks\n","\n","import spacy\n","\n","nlp = spacy.load('en') # load English module\n"],"metadata":{"id":"l7nSdkPruFQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now let's take our single_talk_upd object,\n","# process it through the spacy's English module,\n","# and tokenize it!\n","\n","doc = nlp(single_talk_upd)\n","\n","tokens = [e.text for e in doc]\n","\n","tokens\n"],"metadata":{"id":"Y4JZ3ZTUtF9G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [token.text for token in doc] - this is called 'list comprehension'\n","# List comprehension is a shortened way to write a for loop in which you get a list with elements as an output\n","\n","# Here is how you can obtain the same result by using a full for loop\n","\n","tokens_2 = []\n","\n","for e in doc: # for each element in doc..\n","\n","  token = e.text # tokenize it via spacy's .text method..\n","  \n","  tokens_2.append(token) # ..and append it to the tokens_2 list\n"],"metadata":{"id":"51XDtlrzxM-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokens and tokens_2 are identical\n","\n","tokens == tokens_2 # True\n"],"metadata":{"id":"P9lNuIf2zAJc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note some of the features of tokenization:\n","\n","+ each punctuation symbol is a separate token e.g. `)`, `.`, `?`\n","+ contractions are considered as distinc tokens e.g. `we're` -> `we'` and `'re`"],"metadata":{"id":"FJW81TYvzelg"}},{"cell_type":"markdown","source":["### **Step 3: Stop words removal**\n","\n","**Stop words** are those words that lack meaning and/or aren't much of use for the purposes of the analysis\n","\n","<figure>\n","<left>\n","<img src=https://www.mediavine.com/wp-content/uploads/2020/04/stop-words-infographic-2.jpg.webp width=\"600\">\n","</figure>\n","\n","[Image source](https://www.mediavine.com/stop-words/)\n","\n"],"metadata":{"id":"ysKDwdq-1ItT"}},{"cell_type":"code","source":["# spacy has its own list of stop words and we are going to use it to clean up our string\n","\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","stop = STOP_WORDS\n","\n","stop\n","\n","# Note that this list is tailored to deal with tokenized string\n","# as it contains such tokens as  \"'ve\", \"'ll\", etc.\n"],"metadata":{"id":"S8PFzMGe185P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's create an updated list of tokens, in which all the stop words are excluded\n","\n","tokens_upd = [e.text for e in doc if e.is_stop == False]\n","\n","tokens_upd\n"],"metadata":{"id":"SIXswwy421Kx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Same as:\n","\n","tokens_upd_2 = []\n","\n","for e in doc: # for each element in doc..\n","\n","  if e.is_stop == False: # ..if it's not in the stop word list..\n","\n","    token = e.text # ..tokenize it via spacy's .text method..\n","\n","    tokens_upd_2.append(token) # ..and append it to the tokens_upd_2 list\n"],"metadata":{"id":"NuPM42Cr3Xcm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokens_upd and tokens_upd_2 are identical\n","\n","tokens_upd == tokens_upd_2\n"],"metadata":{"id":"t8mkkaE_5Bdm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Additionally, let's get rid of all the punctuation tokens\n","\n","tokens_upd = [e.text for e in doc if e.is_stop == False and e.text.isalpha() == True]\n","\n","tokens_upd\n","\n","# .isalpha() returns True if all the characters in the string are alphabet letters (a-z)\n","# .isalnum() returns True if all the characters in the string are alphanumeric (a-z0-9)\n"],"metadata":{"id":"s8IG5ayn7IOn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Step 4: Lemmatization**\n","\n","Check out [this](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) page to get more information on **lemmatization** and **stemming**\n","\n","The goal of both **lemmatization** and **stemming** is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form\n","\n","**Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes\n","\n","**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the *lemma*\n","\n","<figure>\n","<left>\n","<img src=https://miro.medium.com/max/1400/1*ES5bt7IoInIq2YioQp2zcQ.png width=\"600\">\n","</figure>\n","\n","[Image source](https://medium.com/geekculture/introduction-to-stemming-and-lemmatization-nlp-3b7617d84e65)\n","\n"],"metadata":{"id":"nADY7FCS82NG"}},{"cell_type":"code","source":["# Let's lemmatize our string\n","\n","lemmas = [e.lemma_ for e in doc if e.is_stop == False and e.text.isalpha() == True]\n","\n","lemmas\n"],"metadata":{"id":"OSKBluCR8nbE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# One of the ways to ensure that lemmatization has been done correctly\n","# is to check whether the length of token list is the same as the length of lemmas list\n","\n","len(tokens_upd) == len(lemmas) # True\n"],"metadata":{"id":"6LLOs2wrPDIc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now let's quickly create a dataframe with two columns to see which lemmas have been derived from each token\n","\n","tokens_lemmas = pd.DataFrame({'token' : tokens_upd,\n","                              'lemma' : lemmas})\n"],"metadata":{"id":"J_aTDxpG9wtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This will ensure that all rows of the dataframe will be shown \n","\n","pd.set_option('display.max_rows', None)\n"],"metadata":{"id":"nTeKVrPm-IB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calling the dataframe\n","\n","tokens_lemmas\n"],"metadata":{"id":"KiOFYeIVPwJy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Comparing the original and preprocessed transcripts**\n"],"metadata":{"id":"eaXwmgWfWW9O"}},{"cell_type":"code","source":["# Roughly speaking, we preprocessed this original TED talk transcript:\n","\n","single_talk\n"],"metadata":{"id":"EXx0cwmLQDXM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Into this:\n","\n","' '.join(lemmas)\n"],"metadata":{"id":"fZv5vrwrQQj9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's quickly draw two word clouds to visually compare them!\n","\n","from wordcloud import WordCloud, STOPWORDS\n","\n","import matplotlib.pyplot as plt # data visualization library\n"],"metadata":{"id":"bPO2M6QiQmia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This one is for the original transcript\n","\n","# Note that it's still a bit preprocessed via WordCloud arguments\n","# stopwords are excluded and plurals are normalized\n","\n","wordcloud_original = WordCloud(background_color = 'white',\n","                     width = 2000, # width of canvas\n","                     height = 1000, # height of canvas\n","                     stopwords = STOPWORDS, # the built-in STOPWORDS list is used\n","                     collocations = False, # whether to include collocations (bigrams) of two words\n","                     normalize_plurals = True, # e.g. 'day' and 'days' will be counted as one\n","                     random_state = 1, # seed to get exactly same wordcloud every time you rerun script\n","                     colormap = 'seismic') # set the colormap\n","\n","# Generate a wordcloud on a string object\n","wordcloud_original.generate(single_talk)\n"],"metadata":{"id":"QdJYSfmmPqEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# And this one is for the preprocessed transcript\n","\n","wordcloud_preprocessed = WordCloud(background_color = 'white',\n","                         width = 2000, # width of canvas\n","                         height = 1000, # height of canvas\n","                         stopwords = STOPWORDS, # the built-in STOPWORDS list is used\n","                         collocations = False, # whether to include collocations (bigrams) of two words\n","                         normalize_plurals = True, # e.g. 'day' and 'days' will be counted as one\n","                         random_state = 1, # seed to get exactly same wordcloud every time you rerun script\n","                         colormap = 'seismic') # set the colormap\n","\n","# Generate a wordcloud on a string object\n","wordcloud_preprocessed.generate(' '.join(lemmas))\n"],"metadata":{"id":"rsLaGO7-XiGM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drawing both wordclouds on one plot\n","\n","figure, (ax1, ax2) = plt.subplots(1, 2, figsize=(28, 23), sharex = 'all', sharey = 'all')\n","\n","ax1.axis('off')\n","ax1.set_title('Original transcript', fontsize = 25)\n","ax1.imshow(wordcloud_original)\n","\n","ax2.axis('off')\n","ax2.set_title('Preprocessed transcript', fontsize = 25)\n","ax2.imshow(wordcloud_preprocessed)\n","\n","plt.show()\n"],"metadata":{"id":"WQ3fMzaUSTA3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Wrapping up all the preprocessing steps into a single function**\n","\n","Our preprocessing pipeline consists of **5 steps**:\n","\n","1. Making text lowercase\n","2. Splitting it into tokens\n","3. Removing stop words\n","4. Removing punctuation\n","5. Lemmatizing tokens\n"],"metadata":{"id":"XQbIE0HKWxDx"}},{"cell_type":"code","source":["# Defining a preprocessing function\n","\n","def preprocess(string):\n","\n","  # making text lowercase\n","  string_low = string.lower()\n","\n","  # processing lowercase text through spacy's English module\n","  doc = nlp(string_low)\n","\n","  # obtaining token lemmas via 1) splitting into tokens, 2) removing stop words, 3) removing punctuation\n","  lemmas = [e.lemma_ for e in doc if e.is_stop == False and e.text.isalpha() == True]\n","\n","  # returning lemmas\n","  return(lemmas)\n"],"metadata":{"id":"45EA3fKWXEsb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's try out this function on a subset of TED talks!\n","\n","# Creating a subset of first 5 TED talks\n","\n","ted_subset = df.iloc[0:5, 0]\n","\n","ted_subset\n"],"metadata":{"id":"qiCndDPjZpJI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Applying the preprocess function onto the subset of TED talks\n","\n","ted_lemmas = ted_subset.apply(lambda x: preprocess(x))\n"],"metadata":{"id":"19s2CT77Zyy6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# What you get as an output is pandas Series (array), in which each value is a list of lemmas\n","\n","type(ted_lemmas) # pandas.core.series.Series\n","\n","ted_lemmas\n"],"metadata":{"id":"RAY8u1Uiegut"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If you want to convert this Series into a list (to get a list of lists), use .tolist() method\n","\n","ted_lemmas_list = ted_lemmas.tolist()\n"],"metadata":{"id":"oPjU4BaReu6W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(ted_lemmas_list) # list\n","\n","len(ted_lemmas_list) # 5 sublists within a list\n","\n","ted_lemmas_list[0] # first sublist, that is a list of lemmas for the first TED talk\n","\n","ted_lemmas_list[0][0] # first element of the first sublist, that is the first lemma for the first TED talk\n","\n","len(ted_lemmas_list[0]) # 609 elements in the first sublist, that is 609 lemmas in the first TED talk\n"],"metadata":{"id":"-vyO9kEPe-N3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HZzDraYdy862"},"source":["## **Exercise**\n","\n","*Get the longest TED talk from the dataset and carefully apply all the preprocessing steps to it!*\n","\n","\n","\n"]},{"cell_type":"code","source":["# How to get the longest TED talk?\n","\n","df['transcript'].apply(lambda x: len(x)).max() # the longest transcript contains 30429 characters\n","\n","df['transcript'].apply(lambda x: len(x)).idxmax() # its index is 65!\n"],"metadata":{"id":"5KKimgHRft6N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save it as a separate object\n","\n","longest_transcript = df['transcript'][65]\n","\n","longest_transcript\n"],"metadata":{"id":"6XZZaAGjcy1o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply your preprocessing steps here:\n","\n"],"metadata":{"id":"qn9kvAJrjP7q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M8g6p6TF8C_A"},"source":["# **That's the end of Day 3 Part 2!**"]}]}