{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Intro_to_text_analysis_Day_4_Part_1.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1J_VwH0BZl_NnrWBHnK-cScNcjzQWZXEG","authorship_tag":"ABX9TyPEGn0LVMXsqELssYPvLu9w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EWB457PFT4J2"},"source":["# **Introduction to text analysis in Python. Day 4 Part 1**\n","\n","## *Dr Kirils Makarovs*\n","\n","## *k.makarovs@exeter.ac.uk*\n","\n","## *University of Exeter Q-Step Centre*\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"6b_C8WzNqV4z"},"source":["# **Welcome to Day 4 Part 1!**"]},{"cell_type":"markdown","metadata":{"id":"DmD95g7Na2wJ"},"source":["## **Today, we are going to look at:**\n","\n","+ *Bag-of-Words* model and `CountVectorizer`\n","+ Lexicon-based sentiment analysis\n","\n","---\n","\n"]},{"cell_type":"markdown","source":["## **Preparatory steps first**"],"metadata":{"id":"RK2l5IdsoWxI"}},{"cell_type":"code","source":["# Importing some of the required libraries\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n"],"metadata":{"id":"PWyNUp18oTL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This will ensure that all rows of the dataframe will be shown \n","\n","pd.set_option('display.max_rows', None)\n"],"metadata":{"id":"NyySfKQdxQTF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Uploading the dataset containing TED talks into the current Google Colab session\n","\n","from google.colab import files\n","\n","uploaded = files.upload()\n"],"metadata":{"id":"rc_6ziYfoTL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting the dataset\n","\n","df = pd.read_csv('ted.csv')\n"],"metadata":{"id":"dLPJX8gxoTL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's take a single TED talk transcript and preprocess it!\n","\n","single_talk = df['transcript'][0]\n","\n","single_talk\n"],"metadata":{"id":"WGKTnzE8oTL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We will use spacy library for this and other tasks\n","\n","import spacy\n","\n","nlp = spacy.load('en') # load English module\n"],"metadata":{"id":"PXok1OXFo07A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defining a preprocessing function\n","\n","def preprocess(string):\n","\n","  # making text lowercase\n","  string_low = string.lower()\n","\n","  # processing lowercase text through spacy's English module\n","  doc = nlp(string_low)\n","\n","  # obtaining token lemmas via 1) splitting into tokens, 2) removing stop words, 3) removing punctuation\n","  lemmas = [e.lemma_ for e in doc if e.is_stop == False and e.text.isalpha() == True]\n","\n","  # glue lemmas back into a string\n","  lemmas_to_string = ' '.join(lemmas)\n","\n","  # returning lemmas\n","  return(lemmas_to_string)\n"],"metadata":{"id":"IBzcyxVFpBgd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing the single_talk object\n","\n","single_talk_prep = preprocess(single_talk)\n","\n","single_talk_prep\n","\n","type(single_talk_prep) # str\n"],"metadata":{"id":"CixKvFU9pYJI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting this string object into a list,\n","# as CountVectorizer requires a list or other iterable (e.g. pandas Series) as an input\n","\n","single_talk_prep_list = [single_talk_prep]\n","\n","type(single_talk_prep_list) # list\n"],"metadata":{"id":"jZJBcrYn2r0L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ygnZsPQr3-mL"},"source":["# **1. *Bag-of-Words* model and `CountVectorizer`**\n","\n","`CountVectorizer` is Python tool to transform text into a **Bag-of-Words** model\n","\n","**Bag-of-Words** model is essentially a matrix that looks like that:\n","\n","<figure>\n","<left>\n","<img src=https://miro.medium.com/max/880/1*hLvya7MXjsSc3NS2SoLMEg.png  width=\"600\">\n","</figure>\n","\n","[Image source](https://medium.com/swlh/spam-filtering-using-bag-of-words-aac778e1ee0b)\n","\n","In this matrix, each row is a **document**, and each column is a **token**\n","\n","The values in the matrix cells show the frequency of each token in a document\n","\n","**Bag-of-Words** matrix can be used:\n","\n","+ as an input to the machine learning models\n","+ as a handy tool to count the occurence of tokens per document\n","\n","*Remember: the better text is preprocessed, the more accurate the model is going to be!*\n","\n","\n"]},{"cell_type":"code","source":["# Import the CountVectorizer from the sklearn library\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n"],"metadata":{"id":"okFwBP2wp2Hr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instantiate the vectorizer\n","\n","# Check out this page for all the parameters that you can modify within CountVectorizer():\n","# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n","\n","count_vector = CountVectorizer()\n","\n","# Fit vectorizer onto the preprocessed TED talk and transform it into a Bag-of-Words\n","\n","bow_model = count_vector.fit_transform(single_talk_prep_list)\n"],"metadata":{"id":"zcRtBflBp_Ll"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CountVectorizer return an object called sparse matrix\n","\n","# Sparse matrix is essentially a matrix that contains a lot of zeros,\n","# and keeping it in a separate type of object allows Python to handle it more efficiently\n","\n","type(bow_model) # scipy.sparse.csr.csr_matrix\n"],"metadata":{"id":"4QwH13vLrPZc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bow_model.shape # 1 row, 316 columns\n","\n","# This means that 1 document has been broken down into 316 elements (lemmas)\n"],"metadata":{"id":"kjPZfYdMr3nK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show all lemmas in the vocabulary\n","\n","count_vector.vocabulary_ # this is a dictionary object\n","\n","# let's keep lemmas as a separate array\n","\n","lemmas = count_vector.get_feature_names_out()\n","\n","len(lemmas) # 316 lemmas\n"],"metadata":{"id":"DvQshOKbsUA3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# You can convert the Bag-of-Words matrix into a single array by using the .toarray() method\n","\n","frequencies = bow_model.toarray()\n","\n","frequencies\n"],"metadata":{"id":"O3csZTYpt1kF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Recall that in the Bag-of-Words matrix, each row is a document, and each column as a lemma\n","\n","# Let's recreate this matrix!\n","\n","bow_df = pd.DataFrame(frequencies, # the values in the dataframe are taken from the frequencies object\n","                      columns = lemmas, # column names are taken from the lemmas object\n","                      index = ['First TED talk']) # you can additionally give a name to an index if you wish\n","\n","bow_df\n"],"metadata":{"id":"UenjospUCtRK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finally, let's see what are the most common words in this document!\n","\n","bow_df.transpose().sort_values('First TED talk', ascending = False).head(15)\n","\n","bow_df.transpose().sort_values('First TED talk', ascending = False).head(15)\n","\n","\n","# The most frequent words are 'illusion', 'go', 'thing', and 'sort'!\n"],"metadata":{"id":"sure3B2EERJv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Now running the same thing but on the subset of first 50 TED talks!**\n","\n","(it might take quite a while if you run this on the entire dataframe of 500 talks)"],"metadata":{"id":"MVO8F1zuxwk1"}},{"cell_type":"code","source":["# Additionally preprocessing all talks to save as a separate object\n","\n","ted_clean = df['transcript'].apply(lambda x: preprocess(x))\n","\n","ted_clean\n"],"metadata":{"id":"9lvhVWN3rwQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving it as a separate .csv file\n","\n","ted_clean.to_csv('ted_clean.csv', index = False)\n"],"metadata":{"id":"YCL6C8Eysjym"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing first 50 talks\n","\n","ted_clean = df['transcript'][0:50].apply(lambda x: preprocess(x))\n","\n","ted_clean\n"],"metadata":{"id":"mjlUjZPexqTd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instantiate the vectorizer\n","\n","count_vector = CountVectorizer()\n","\n","# Fit vectorizer onto the preprocessed TED talks and transform them into a Bag-of-Words\n","# Note that CountVectorizer() accepts pandas Series with strings, so there is no need to transform it into a list\n","\n","bow_model = count_vector.fit_transform(ted_clean)\n","\n","\n"],"metadata":{"id":"5KL7JYQbzMjF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CountVectorizer return an object called sparse matrix\n","\n","# Sparse matrix is essentially a matrix that contains a lot of zeros,\n","# and keeping it in a separate type of object allows Python to handle it more efficiently\n","\n","type(bow_model) # scipy.sparse.csr.csr_matrix\n"],"metadata":{"id":"l_VXLbAvFfka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bow_model.shape # 50 rows, 6446 columns\n","\n","# This means that 50 documents have been broken down into 6446 unique elements (lemmas)\n"],"metadata":{"id":"KEs6ASshFfkd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show all lemmas in the vocabulary\n","\n","count_vector.vocabulary_ # this is a dictionary object\n","\n","# let's keep lemmas as a separate array\n","\n","lemmas = count_vector.get_feature_names_out()\n","\n","len(lemmas) # 6446 lemmas\n"],"metadata":{"id":"5dqTEqxiFfkd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# You can convert the Bag-of-Words matrix into an array with lists by using the .toarray() method\n","\n","frequencies = bow_model.toarray()\n","\n","frequencies\n"],"metadata":{"id":"cPoQWaqOFfke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Recall that in the Bag-of-Words matrix, each row is a document, and each column as a lemma\n","\n","# Let's recreate this matrix!\n","\n","bow_df = pd.DataFrame(frequencies, # the values in the dataframe are taken from the frequencies object\n","                      columns = lemmas) # column names are taken from the lemmas object\n","\n","bow_df.head(10)\n","\n","# Each row is a document, and each column is a unique lemma!\n"],"metadata":{"id":"PXJMKhnBFfke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adding row names for clarity\n","\n","row_names = []\n","\n","for e in np.arange(1, 51):\n","  \n","  row = 'Talk #' + str(e) # Talk #1, Talk #2, Talk #3, ... , Talk #50\n","\n","  row_names.append(row)\n","\n","bow_df.index = row_names\n","\n","bow_df.head(10)\n"],"metadata":{"id":"RlPW5YnQGZzu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finally, let's see what are the most common words across all these documents!\n","\n","# Since there are more than 1 row (document) in this matrix,\n","# we need to manually calculate the frequency of each lemma across all documents\n","\n","freq_total = bow_df.sum(axis = 0) # sum up all values by row (axis = 1 would sum up by column)\n","\n","len(freq_total) # 6446, so we got a frequency value for each lemma, this is exactly what we wanted!\n"],"metadata":{"id":"3nmQcS6JIZ6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finally, we can order this array\n","# and see what are the 50 most common words (lemmas) across all these TED talks!\n","\n","freq_total.sort_values(ascending = False).head(50)\n"],"metadata":{"id":"Odgs8ztOLQoP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the distribution of lemmas? Yes!\n","\n","plt.figure(figsize = (14, 9)) # set figure size\n","\n","# To understand what is going on here, break down this code bit by bit,\n","# run it, and see what you get as an output of each step\n","freq_total.sort_values(ascending = False).head(25).iloc[::-1].plot(kind = 'barh',\n","                                                                   color = 'green')\n","\n","plt.title('The frequency of 25 most popular lemmas\\n in 50 TED talks', fontsize = 25)\n","plt.xlabel('Frequency', fontsize = 20)\n","plt.ylabel('Lemma', fontsize = 20) \n","\n","plt.xticks(ticks = np.arange(0, 400, 25), fontsize = 15) # tweak x axis ticks\n","plt.yticks(fontsize = 15) # tweak y axis ticks\n","\n","plt.show()\n"],"metadata":{"id":"MzrtWLh6PEI_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8rPV5yDQXMxR"},"source":["## **Exercise**\n","\n","Remember that newspaper article that we inspected in the first class? \n","\n","It was entitled *Overconfident of spotting fake news? If so, you may be more likely to fall victim*.\n","\n","Please: \n","+ *preprocess it*\n","+ *obtain the Bag-of-Words model*\n","+ *see what are the most common words (lemmas) that are used in it*\n","\n","You can find an original article [here](https://www.theguardian.com/media/2021/may/31/confident-spotting-fake-news-if-so-more-likely-fall-victim)\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["# Newspaper article (no preprocessing or text cleaning has been done)\n","\n","article = 'Are you a purveyor of fake news? People who are most confident about their ability to discern between fact and fiction are also the most likely to fall victim to misinformation, a US study suggests. Although Americans believe the confusion caused by false news is all-pervasive, relatively few indicate having seen or shared it, something the researchers suggested shows that many may not only have a hard time identifying false news but are not aware of their own deficiencies at doing so. Nine out of 10 participants surveyed indicated they were above average in their ability to discern false and legitimate news headlines. About a fifth of respondents rated themselves 50 or more percentiles higher than their score warranted, the analysis of a nationally representative study of data collected during and after the 2018 US midterm elections found. In the survey, 8,285 Americans were asked to evaluate the accuracy of a series of Facebook headlines, and then rate their own abilities in discerning false news content relative to others. When researchers looked at data measuring respondents’ online behaviour, those with inflated perceptions of their abilities more frequently visited websites linked to the spread of false or misleading news. The overconfident participants were also less able to distinguish between true and false claims about current events and reported higher willingness to share false content, especially when it aligned with their political predispositions, the authors found. “No matter what domain, people on average are overconfident … but over 70% of people displaying overconfidence is just such a huge number,” said the lead author, Ben Lyons, an assistant professor of communication at the University of Utah. Although the study does not prove that overconfidence directly causes engagement with false news, the mismatch between a person’s perceived ability to spot misinformation and their actual competence could play a crucial role in the spread of false information, the authors wrote in the studypublished in the Proceedings of the National Academy of Sciences of the United States of America. It also suggests that those who are humble – people who tend to engage in self-monitoring, reflective behaviours and put more thought into the sites they visit and content they share – are likely to be less susceptible to misinformation, said Lyons. Factors such as gender also played a key role in the likelihood of overconfidence and, in turn, vulnerability to false news, suggested Lyons. “Male respondents [in the study] displayed more overconfidence – and this is a consistent finding in overconfidence literature – men are always more confident than women, which is always not so surprising.” He added: “Overconfidence is truly universal. I would be shocked if we didn’t find this in every country we looked at … although we might not see this extreme level of overconfidence, just based on cultural differences.”'\n","\n","article\n"],"metadata":{"id":"PzYThMP9P9hA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M8g6p6TF8C_A"},"source":["# **That's the end of Day 4 Part 1!**"]}]}